{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "!pip install contractions\n",
        "!pip install pyspellchecker\n",
        "!pip install datasets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qWZ6sS4h5J7",
        "outputId": "8e4d5a88-8cc7-461e-d896-02c3451f6814"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FNk7xpSKgm3x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import contractions  # Import the contractions library\n",
        "from spellchecker import SpellChecker  # Import the SpellChecker library\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet #import wordnet for POS tagging\n",
        "from datasets import load_dataset\n",
        "import spacy\n",
        "import shutil\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('omw-1.4')  # Optional but useful for lemmatization\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.data.find('taggers/averaged_perceptron_tagger.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaEoqYoShCV8",
        "outputId": "c294606a-9463-4769-afd3-8b6b2412aabe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ZipFilePathPointer('/root/nltk_data/taggers/averaged_perceptron_tagger.zip', '')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stopwords once\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize the spellchecker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Load spaCy English model ✅\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def expand_clitics(text):\n",
        "    \"\"\"Expand contractions like \"I'm\" → \"I am\" \"\"\"\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def correct_spelling(tokens):\n",
        "    \"\"\"Spell check each word\"\"\"\n",
        "    return [spell.correction(word) if spell.correction(word) else word for word in tokens]\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Convert text to lowercase, expand contractions, and remove punctuation\"\"\"\n",
        "    text = text.lower()\n",
        "    text = expand_clitics(text)\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Tokenize text\"\"\"\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \"\"\"Perform lemmatization using spaCy with accurate POS tagging\"\"\"\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "def load_slang_mapping(file_path):\n",
        "    \"\"\"Load the slang mapping from the provided CSV file\"\"\"\n",
        "    slang_df = pd.read_csv(file_path, usecols=[1, 2], names=['acronym', 'expansion'], skiprows=1)\n",
        "    slang_mapping = dict(zip(slang_df['acronym'].str.lower(), slang_df['expansion'].str.lower()))\n",
        "    return slang_mapping\n",
        "\n",
        "def replace_slang(tokens, slang_mapping):\n",
        "    \"\"\"Replace slang words with their expansions\"\"\"\n",
        "    doc = nlp(' '.join(tokens))\n",
        "    updated_tokens = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.text.lower() == 'am' and token.pos_ == 'AUX':\n",
        "            updated_tokens.append(token.text)\n",
        "        elif token.text.lower() in slang_mapping:\n",
        "            expansion_tokens = [t.text for t in nlp(slang_mapping[token.text.lower()])]\n",
        "            updated_tokens.extend(expansion_tokens)\n",
        "        else:\n",
        "            updated_tokens.append(token.text)\n",
        "\n",
        "    return updated_tokens\n",
        "\n",
        "def get_pos_tags(tokens):\n",
        "    \"\"\"Get POS tags for each token using spaCy\"\"\"\n",
        "    doc = nlp(' '.join(tokens))\n",
        "    return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "def preprocess_data(file_path, slang_file_path):\n",
        "    \"\"\"Main preprocessing function\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Load slang mapping\n",
        "    slang_mapping = load_slang_mapping(slang_file_path)\n",
        "\n",
        "    # Clean and process text\n",
        "    df['cleaned_text'] = df['Utterance'].apply(clean_text)\n",
        "    df['tokenized_text'] = df['cleaned_text'].apply(tokenize_text)\n",
        "\n",
        "    # Replace slang words\n",
        "    df['slang_replaced_text'] = df['tokenized_text'].apply(lambda tokens: replace_slang(tokens, slang_mapping))\n",
        "\n",
        "    # Apply spell checking\n",
        "    df['spell_checked_text'] = df['slang_replaced_text'].apply(correct_spelling)\n",
        "\n",
        "    # Remove stopwords\n",
        "    df['filtered_text'] = df['spell_checked_text'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
        "\n",
        "    # Apply stemming\n",
        "    df['stemmed_text'] = df['filtered_text'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n",
        "\n",
        "    # Apply lemmatization using spaCy (POS tagging)\n",
        "    df['lemmatized_text'] = df['cleaned_text'].apply(lemmatize_text)\n",
        "\n",
        "     # Get POS tags for each tokenized text\n",
        "    df['pos_tags'] = df['slang_replaced_text'].apply(lambda tokens: get_pos_tags(tokens))\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeZyvdB1lD6q",
        "outputId": "b2fcc6b9-32d3-4dc6-b91c-3be5dc86a434"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (only need to do this once per session)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNwu6XRGhLLx",
        "outputId": "ee9a8cbf-df04-44dc-acbe-62cf798b6202"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify file paths in Google Drive\n",
        "file_path = '/content/drive/MyDrive/BTECH_PROJECT/abc.csv'  # Update path as necessary\n",
        "output_path = '/content/drive/MyDrive/BTECH_PROJECT/preprocessed_data_abc.csv'  # Update path as necessary\n",
        "slang_file_path = '/content/drive/MyDrive/BTECH_PROJECT/slang.csv'\n",
        "# Run preprocessing\n",
        "df_preprocessed = preprocess_data(file_path, slang_file_path)\n",
        "\n",
        "# Save and display results\n",
        "df_preprocessed.to_csv(output_path, index=False)\n",
        "print(df_preprocessed[['Utterance', 'cleaned_text', 'tokenized_text','slang_replaced_text',  'spell_checked_text', 'lemmatized_text', 'pos_tags']].head())  # Display the original, cleaned, tokenized, and spell-checked columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rpyy9bUhMA6",
        "outputId": "78ff083c-ba9d-4c89-9a34-a089d862c926"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           Utterance  \\\n",
            "0                                 Are u doing today?   \n",
            "1    Alright, thanks. How are you? What is the time?   \n",
            "2  I'm doing okay. Thx for asking. It is 10 am. I...   \n",
            "3                                         Yeah, brb    \n",
            "4                               Can lmk about those?   \n",
            "\n",
            "                                        cleaned_text  \\\n",
            "0                                are you doing today   \n",
            "1        alright thanks how are you what is the time   \n",
            "2  i am doing okay thanks for asking it is 10 am ...   \n",
            "3                                          yeah brb    \n",
            "4                                can lmk about those   \n",
            "\n",
            "                                      tokenized_text  \\\n",
            "0                           [are, you, doing, today]   \n",
            "1  [alright, thanks, how, are, you, what, is, the...   \n",
            "2  [i, am, doing, okay, thanks, for, asking, it, ...   \n",
            "3                                        [yeah, brb]   \n",
            "4                           [can, lmk, about, those]   \n",
            "\n",
            "                                 slang_replaced_text  \\\n",
            "0                           [are, you, doing, today]   \n",
            "1  [alright, thanks, how, are, you, what, is, the...   \n",
            "2  [i, am, doing, okay, thanks, for, asking, it, ...   \n",
            "3                            [yeah, be, right, back]   \n",
            "4                 [can, let, me, know, about, those]   \n",
            "\n",
            "                                  spell_checked_text  \\\n",
            "0                           [are, you, doing, today]   \n",
            "1  [alright, thanks, how, are, you, what, is, the...   \n",
            "2  [i, am, doing, okay, thanks, for, asking, it, ...   \n",
            "3                            [yeah, be, right, back]   \n",
            "4                 [can, let, me, know, about, those]   \n",
            "\n",
            "                                     lemmatized_text  \\\n",
            "0                               [be, you, do, today]   \n",
            "1  [alright, thank, how, be, you, what, be, the, ...   \n",
            "2  [I, be, do, okay, thank, for, ask, it, be, 10,...   \n",
            "3                                        [yeah, brb]   \n",
            "4                           [can, lmk, about, those]   \n",
            "\n",
            "                                            pos_tags  \n",
            "0  [(are, AUX), (you, PRON), (doing, VERB), (toda...  \n",
            "1  [(alright, INTJ), (thanks, NOUN), (how, SCONJ)...  \n",
            "2  [(i, PRON), (am, AUX), (doing, VERB), (okay, I...  \n",
            "3  [(yeah, INTJ), (be, AUX), (right, ADV), (back,...  \n",
            "4  [(can, AUX), (let, VERB), (me, PRON), (know, V...  \n"
          ]
        }
      ]
    }
  ]
}